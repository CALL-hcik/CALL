{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RQ1: Question Quality Check\n",
    "\n",
    "This notebook parses generated questions and evaluates their quality using LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import glob\n",
    "import time\n",
    "import re\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import google.generativeai as genai\n",
    "from openai import OpenAI\n",
    "import anthropic\n",
    "\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\", \"YOUR_GOOGLE_API_KEY_HERE\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\", \"YOUR_OPENAI_API_KEY_HERE\")\n",
    "ANTHROPIC_API_KEY = os.getenv(\"ANTHROPIC_API_KEY\", \"YOUR_ANTHROPIC_API_KEY_HERE\")\n",
    "\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "TOPIC_KEYWORDS = [\n",
    "    \"정권 교체\", \"통합 정치\", \"단일화(윤석열-안철수)\",\n",
    "    \"부동산, 세금 등 경제문제\", \"여성가족부 폐지\",\n",
    "    \"후보(또는 가족)의 비리\", \"대장동 의혹\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_llm(prompt, model_type=\"gemini\", **kwargs):\n",
    "    if model_type == \"gemini\":\n",
    "        model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
    "        response = model.generate_content(prompt)\n",
    "        return response.text\n",
    "    elif model_type == \"gpt\":\n",
    "        client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            **kwargs\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    elif model_type == \"claude\":\n",
    "        client = anthropic.Anthropic(api_key=ANTHROPIC_API_KEY)\n",
    "        message = client.messages.create(\n",
    "            model=\"claude-3-5-haiku-20241022\",\n",
    "            max_tokens=kwargs.get(\"max_tokens\", 1000),\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "        return message.content[0].text\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model_type: {model_type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse Generated Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_questions_from_file(filepath):\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "    \n",
    "    data = []\n",
    "    questions = re.findall(r\"Q\\d+: (.*?)(?=Q\\d+:|$)\", text, re.DOTALL)\n",
    "    \n",
    "    for q in questions:\n",
    "        question_part = q.split(\"Answer\")[0].strip()\n",
    "        fm = re.search(r\"에펨코리아: ([A-D])\", q)\n",
    "        mlb = re.search(r\"MLBPARK: ([A-D])\", q)\n",
    "        pp = re.search(r\"뽐뿌: ([A-D])\", q)\n",
    "        \n",
    "        if fm and mlb and pp:\n",
    "            data.append({\n",
    "                \"question\": question_part,\n",
    "                \"fm\": fm.group(1),\n",
    "                \"mlb\": mlb.group(1),\n",
    "                \"pp\": pp.group(1)\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "for topic in TOPIC_KEYWORDS:\n",
    "    for model_type in [\"gemini\", \"gpt\", \"claude\"]:\n",
    "        pattern = f\"../dataset/RQ1_questions/{topic}/output/{model_type}/output_*.txt\"\n",
    "        files = glob.glob(pattern)\n",
    "        \n",
    "        for filepath in files:\n",
    "            df = parse_questions_from_file(filepath)\n",
    "            \n",
    "            output_dir = \"../dataset/RQ1_questions/parsed_questions\"\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "            \n",
    "            file_name = os.path.basename(filepath).replace(\".txt\", \"\")\n",
    "            output_file = f\"{output_dir}/{topic}_{model_type}_{file_name}.csv\"\n",
    "            df.to_csv(output_file, index=False, encoding=\"utf-8-sig\")\n",
    "            print(f\"Saved: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question Quality Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANSWER_CONSISTENCY_PROMPT = \"\"\"\n",
    "Evaluate the quality of the generated question.\n",
    "Generated question and answer: <{question}>\n",
    "\n",
    "Score 1: The question cannot be answered by the provided answer.\n",
    "Score 2: The question can be partially answered using the provided answer.\n",
    "Score 3: The question can be answered directly using the provided answer.\n",
    "\n",
    "Return only the score (1, 2, or 3).\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "\n",
    "for eval_model in [\"gemini\", \"gpt\", \"claude\"]:\n",
    "    print(f\"\\nEvaluating with {eval_model}...\")\n",
    "    answer_consistency_dict = {}\n",
    "    \n",
    "    for topic in TOPIC_KEYWORDS:\n",
    "        files = glob.glob(f\"../dataset/RQ1_questions/parsed_questions/{topic}_*.csv\")\n",
    "        \n",
    "        for file in tqdm(files, desc=f\"{topic}\"):\n",
    "            df = pd.read_csv(file)\n",
    "            \n",
    "            for _, row in df.iterrows():\n",
    "                question = row[\"question\"]\n",
    "                prompt = ANSWER_CONSISTENCY_PROMPT.format(question=question)\n",
    "                response = call_llm(prompt, model_type=eval_model)\n",
    "                answer_consistency_dict[question] = response\n",
    "                time.sleep(5)\n",
    "    \n",
    "    results[eval_model] = answer_consistency_dict\n",
    "    print(f\"Completed {eval_model}: {len(answer_consistency_dict)} evaluations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Evaluation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"../dataset/RQ1_questions/quality_evaluation\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for model_type, eval_dict in results.items():\n",
    "    output_file = f\"{output_dir}/quality_scores_{model_type}.json\"\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(eval_dict, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"Saved {model_type} evaluations to {output_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}