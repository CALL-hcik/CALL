{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RQ1: Community Opinion Experiment\n",
    "\n",
    "This notebook runs the main RQ1 experiment with Retriever, Summarizer, and Agent modules.\n",
    "Experiments: Ours, RAG, RANDOM, CONV, COMM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import time\n",
    "import glob\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import google.generativeai as genai\n",
    "from openai import OpenAI\n",
    "import anthropic\n",
    "\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\", \"YOUR_GOOGLE_API_KEY_HERE\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\", \"YOUR_OPENAI_API_KEY_HERE\")\n",
    "ANTHROPIC_API_KEY = os.getenv(\"ANTHROPIC_API_KEY\", \"YOUR_ANTHROPIC_API_KEY_HERE\")\n",
    "\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "TOPIC_KEYWORDS = [\n",
    "    \"정권 교체\", \"통합 정치\", \"단일화(윤석열-안철수)\",\n",
    "    \"부동산, 세금 등 경제문제\", \"여성가족부 폐지\",\n",
    "    \"후보(또는 가족)의 비리\", \"대장동 의혹\"\n",
    "]\n",
    "\n",
    "COMMUNITY_MAPPING = {\n",
    "    \"fm\": \"에펨코리아\",\n",
    "    \"mlb\": \"MLBPARK\",\n",
    "    \"pp\": \"뽐뿌\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_llm(prompt, model_type=\"gemini\", **kwargs):\n",
    "    if model_type == \"gemini\":\n",
    "        model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
    "        response = model.generate_content(prompt)\n",
    "        return response.text\n",
    "    elif model_type == \"gpt\":\n",
    "        client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            **kwargs\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    elif model_type == \"claude\":\n",
    "        client = anthropic.Anthropic(api_key=ANTHROPIC_API_KEY)\n",
    "        message = client.messages.create(\n",
    "            model=\"claude-3-5-haiku-20241022\",\n",
    "            max_tokens=kwargs.get(\"max_tokens\", 4096),\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "        return message.content[0].text\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model_type: {model_type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data and FAISS Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "community = \"fm\"\n",
    "\n",
    "df_sample = pd.read_csv(f\"../dataset/community_samples/{community}_sample_50000.csv\", encoding=\"utf-8-sig\")\n",
    "embedding_model = SentenceTransformer(\"intfloat/multilingual-e5-small\", device=\"cuda\")\n",
    "\n",
    "index_path = f\"../dataset/faiss/{community}_embeddings_faiss_50000.index\"\n",
    "if os.path.exists(index_path):\n",
    "    index = faiss.read_index(index_path)\n",
    "    print(f\"Loaded FAISS index: {index.ntotal} vectors\")\n",
    "else:\n",
    "    raise FileNotFoundError(f\"FAISS index not found: {index_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, max_chunk_length=512):\n",
    "    words = text.split()\n",
    "    chunks, current = [], []\n",
    "    for word in words:\n",
    "        if len(\" \".join(current + [word])) <= max_chunk_length:\n",
    "            current.append(word)\n",
    "        else:\n",
    "            chunks.append(\" \".join(current))\n",
    "            current = [word]\n",
    "    if current:\n",
    "        chunks.append(\" \".join(current))\n",
    "    return chunks\n",
    "\n",
    "text_chunks_unique = []\n",
    "seen_chunks = set()\n",
    "\n",
    "for _, row in df_sample.iterrows():\n",
    "    for chunk in chunk_text(str(row[\"Text\"])):\n",
    "        if chunk not in seen_chunks:\n",
    "            seen_chunks.add(chunk)\n",
    "            chunked_row = row.to_dict()\n",
    "            chunked_row[\"Text\"] = chunk\n",
    "            text_chunks_unique.append(chunked_row)\n",
    "\n",
    "chunked_df = pd.DataFrame(text_chunks_unique)\n",
    "chunked_df[\"ProcessedText\"] = chunked_df[\"Text\"].apply(lambda x: f\"query: {x}\")\n",
    "print(f\"Chunked DataFrame: {len(chunked_df)} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retriever Module (augment_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUGMENT_PROMPT = \"\"\"\n",
    "Please write exactly two lines in Korean about the topic '{topic}':\n",
    "1. The first line should briefly state the reason for supporting.\n",
    "2. The second line should briefly state the reason for opposing.\n",
    "\"\"\"\n",
    "\n",
    "def augment_text(topic, top_k, question, model_type=\"gemini\"):\n",
    "    prompt = AUGMENT_PROMPT.format(topic=topic)\n",
    "    aug_response = call_llm(prompt, model_type=model_type)\n",
    "    \n",
    "    query_emb = embedding_model.encode([f\"query: {aug_response}\"], convert_to_numpy=True, normalize_embeddings=True).astype(\"float32\")\n",
    "    distances, indices = index.search(query_emb, top_k)\n",
    "    \n",
    "    retrieved_texts = chunked_df.iloc[indices[0]][\"Text\"].tolist()\n",
    "    return aug_response, retrieved_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarizer Module (extract_opinions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUMMARIZER_PROMPT = \"\"\"\n",
    "You are a sophisticated qualitative analysis AI.\n",
    "Your primary function is to extract and analyze multiple dominant themes from social media posts.\n",
    "Your goal is to identify the various topics, sentiments, and arguments that emerge in public discussions.\n",
    "\n",
    "Core Task: Extract Multiple Dominant Themes\n",
    "Answer the question: \"{question}\" based on the provided posts.\n",
    "Rather than finding a single 'center', identify ALL significant themes that appear in the conversation.\n",
    "\n",
    "Analysis Workflow:\n",
    "1. Initial Scanning: Read all posts to identify distinct themes and topics being discussed.\n",
    "2. Theme Extraction: Identify 2-4 major themes based on frequency, engagement, and significance.\n",
    "\n",
    "Theme Identification Guidelines:\n",
    "- Each theme should be distinct and substantive enough to stand alone\n",
    "- Themes can be completely unrelated to each other\n",
    "- Include both majority and significant minority themes\n",
    "- For each theme, accurately represent its dominant sentiment without artificial balancing\n",
    "\n",
    "Posts:\n",
    "{opinions}\n",
    "\n",
    "Output Format (Korean):\n",
    "<answer>\n",
    "1. [Theme]: [Key points and sentiment summary]\n",
    "2. [Theme]: [Key points and sentiment summary]\n",
    "3. [Theme]: [Key points and sentiment summary]\n",
    "</answer>\n",
    "\"\"\"\n",
    "\n",
    "def extract_opinions(question, texts, model_type=\"gemini\"):\n",
    "    opinions = \"\\n---\\n\".join(texts)\n",
    "    prompt = SUMMARIZER_PROMPT.format(question=question, opinions=opinions)\n",
    "    return call_llm(prompt, model_type=model_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent Module (final_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FINAL_AGENT_PROMPT = \"\"\"\n",
    "Please select your major one opinion below based on the overall opinion. Do nothing else.\n",
    "{persona}\n",
    "\n",
    "<question>\n",
    "{question}\n",
    "\"\"\"\n",
    "\n",
    "def final_agent(analysis, question, model_type=\"gemini\"):\n",
    "    prompt = FINAL_AGENT_PROMPT.format(persona=analysis, question=question)\n",
    "    return call_llm(prompt, model_type=model_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Agents (RAG, Random, Conv, Comm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAG_PROMPT = \"\"\"\n",
    "Please select your major opinion below based on the overall opinion. Do nothing else.\n",
    "{persona}\n",
    "\n",
    "<question>\n",
    "{question}\n",
    "\"\"\"\n",
    "\n",
    "def rag_agent(question, opinions_str, model_type=\"gemini\"):\n",
    "    prompt = RAG_PROMPT.format(persona=opinions_str, question=question)\n",
    "    return call_llm(prompt, model_type=model_type)\n",
    "\n",
    "def random_agent(question, model_type=\"gemini\"):\n",
    "    random_opinions = \"\\n---\\n\".join(chunked_df.sample(n=50)[\"Text\"].values)\n",
    "    prompt = RAG_PROMPT.format(persona=random_opinions, question=question)\n",
    "    return call_llm(prompt, model_type=model_type), random_opinions\n",
    "\n",
    "COMM_PROMPT = \"\"\"\n",
    "You are a member of {community} in South Korea.\n",
    "Please select your opinion based on this community's perspective. Do nothing else.\n",
    "\n",
    "<question>\n",
    "{question}\n",
    "\"\"\"\n",
    "\n",
    "def comm_agent(question, community_name, model_type=\"gemini\"):\n",
    "    prompt = COMM_PROMPT.format(community=community_name, question=question)\n",
    "    return call_llm(prompt, model_type=model_type)\n",
    "\n",
    "CONV_PROMPT = \"\"\"\n",
    "You are a member of conservative group of South Korea.\n",
    "Please select your opinion based on this community's perspective. Do nothing else.\n",
    "\n",
    "<question>\n",
    "{question}\n",
    "\"\"\"\n",
    "\n",
    "def conv_agent(question, model_type=\"gemini\"):\n",
    "    prompt = CONV_PROMPT.format(question=question)\n",
    "    return call_llm(prompt, model_type=model_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_answer_letter(raw_answer_str):\n",
    "    s = str(raw_answer_str).replace(\"\\u00A0\", \" \").strip()\n",
    "    s = re.sub(r\"['\\\"\\u2018\\u2019\\u201c\\u201d`]\", \"\", s)\n",
    "    \n",
    "    match = re.match(r\"^([A-D])\", s, re.IGNORECASE)\n",
    "    if match:\n",
    "        return match.group(1).upper()\n",
    "    \n",
    "    match = re.search(r\"([A-D])[.)]\", s, re.IGNORECASE)\n",
    "    if match:\n",
    "        return match.group(1).upper()\n",
    "    \n",
    "    match = re.search(r\"\\b([A-D])\\b\", s, re.IGNORECASE)\n",
    "    if match:\n",
    "        return match.group(1).upper()\n",
    "    \n",
    "    return None\n",
    "\n",
    "def calculate_accuracy(predictions_df, ground_truth_df, community=\"fm\"):\n",
    "    parsed = predictions_df[\"response\"].apply(parse_answer_letter)\n",
    "    correct = (parsed == ground_truth_df[community]).sum()\n",
    "    total = len(ground_truth_df)\n",
    "    return correct / total if total > 0 else 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_df = pd.read_csv(\"../dataset/RQ1_questions/selected_questions/call_experiment_survey.csv\")\n",
    "top_k = 50\n",
    "num_iterations = 100\n",
    "\n",
    "for model_type in [\"gemini\", \"gpt\", \"claude\"]:\n",
    "    print(f\"\\n=== Running experiments with {model_type} ===\")\n",
    "    \n",
    "    for topic in TOPIC_KEYWORDS:\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        results = []\n",
    "        \n",
    "        topic_questions = questions_df[questions_df[\"topic\"] == topic] if \"topic\" in questions_df.columns else questions_df\n",
    "        \n",
    "        for idx, row in tqdm(topic_questions.iterrows(), desc=f\"{topic}\"):\n",
    "            question = row[\"question\"]\n",
    "            \n",
    "            for seed in range(num_iterations):\n",
    "                aug_text, retrieved = augment_text(topic, top_k, question, model_type=model_type)\n",
    "                summary = extract_opinions(question, retrieved, model_type=model_type)\n",
    "                response = final_agent(summary, question, model_type=model_type)\n",
    "                \n",
    "                results.append({\n",
    "                    \"question\": question,\n",
    "                    \"seed\": seed,\n",
    "                    \"response\": response,\n",
    "                    \"method\": \"ours\"\n",
    "                })\n",
    "                time.sleep(2)\n",
    "        \n",
    "        output_dir = f\"../results/RQ1/{community}\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        pd.DataFrame(results).to_csv(\n",
    "            f\"{output_dir}/{community}_ours_{model_type}_{topic}_{timestamp}.csv\",\n",
    "            index=False, encoding=\"utf-8-sig\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_files = glob.glob(f\"../results/RQ1/{community}/{community}_ours_*.csv\")\n",
    "if result_files:\n",
    "    all_results = pd.concat([pd.read_csv(f, encoding=\"utf-8-sig\") for f in result_files])\n",
    "    \n",
    "    ground_truth = pd.read_csv(\"../dataset/RQ1_questions/selected_questions/call_experiment_survey.csv\")\n",
    "    accuracy = calculate_accuracy(all_results, ground_truth, community=community)\n",
    "    print(f\"Overall accuracy for {community}: {accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
