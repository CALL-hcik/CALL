{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RQ1: Generate Survey Questions from Community Comments\n",
    "\n",
    "This notebook generates multi-choice survey questions from Korean online community comments using LLMs (Gemini, GPT-4o-mini, Claude)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import google.generativeai as genai\n",
    "from openai import OpenAI\n",
    "import anthropic\n",
    "\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\", \"YOUR_GOOGLE_API_KEY_HERE\")\n",
    "\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "TOPIC_KEYWORDS = [\n",
    "    \"정권 교체\", \"통합 정치\", \"단일화(윤석열-안철수)\",\n",
    "    \"부동산, 세금 등 경제문제\", \"여성가족부 폐지\",\n",
    "    \"후보(또는 가족)의 비리\", \"대장동 의혹\"\n",
    "]\n",
    "\n",
    "COMMUNITIES = [\"에펨코리아\", \"MLBPARK\", \"뽐뿌\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_llm(prompt, model_type=\"gemini\", **kwargs):\n",
    "    if model_type == \"gemini\":\n",
    "        model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
    "        response = model.generate_content(prompt)\n",
    "        return response.text\n",
    "    elif model_type == \"gpt\":\n",
    "        client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            **kwargs\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    elif model_type == \"claude\":\n",
    "        client = anthropic.Anthropic(api_key=ANTHROPIC_API_KEY)\n",
    "        message = client.messages.create(\n",
    "            model=\"claude-3-5-haiku-20241022\",\n",
    "            max_tokens=kwargs.get(\"max_tokens\", 4096),\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "        return message.content[0].text\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model_type: {model_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTION_GENERATION_PROMPT = \"\"\"\n",
    "Below are comments from 3 different Korean online communities related to a topic.\n",
    "The topic can be represented using these keywords: {topic_keyword}.\n",
    "\n",
    "{community_comments}\n",
    "\n",
    "Write five questions (Q1-Q5) on the topic:<{topic_keyword}> that can be answered based on these communities.\n",
    "For a community, each question should be answered in a way that the members from the community would do, and the answers should echo the comments shown above.\n",
    "Do NOT rely on your background knowledge about the specific community to answer the questions.\n",
    "The questions should be low-level, detailed, trigger different responses that differentiate between different communities.\n",
    "The questions should not be in the style of reading comprehension ones, and they are intended for members in the community to answer.\n",
    "The questions should not contain \"comment\" in them.\n",
    "Each question should be paired with answers from all 3 communities.\n",
    "The answers should be concise (fewer than 32 tokens), legible, grammatically correct.\n",
    "For the 5 questions, they are multi-choice questions with four options (A through D).\n",
    "\n",
    "Format:\n",
    "Q1: [multi-choice question]\n",
    "A.xxx  B.xxx  C.xxx  D.xxx\n",
    "Answer from Community 에펨코리아: A/B/C/D\n",
    "Answer from Community MLBPARK: A/B/C/D\n",
    "Answer from Community 뽐뿌: A/B/C/D\n",
    "\n",
    "(!Note. Always use Korean)\n",
    "\"\"\"\n",
    "\n",
    "def format_prompt_with_all_comments(communities, comments, topic_keyword):\n",
    "    community_comments = \"\"\n",
    "    for i, community in enumerate(communities):\n",
    "        community_comments += f\"Comments from {community}:\\n\"\n",
    "        for j, comment in enumerate(comments[i]):\n",
    "            community_comments += f\"Comment {j + 1}: {comment}\\n\"\n",
    "        community_comments += \"\\n\"\n",
    "    return QUESTION_GENERATION_PROMPT.format(\n",
    "        topic_keyword=topic_keyword,\n",
    "        community_comments=community_comments.strip()\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Prompts from Community Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = []\n",
    "num_iterations = 5\n",
    "\n",
    "for i in range(num_iterations):\n",
    "    for topic in TOPIC_KEYWORDS:\n",
    "        output_dir = f\"../dataset/prompts/{topic}\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        fm_sample = pd.read_csv(f\"../dataset/question_generation_samples/fm_{topic}.csv\")\n",
    "        mlb_sample = pd.read_csv(f\"../dataset/question_generation_samples/mlb_{topic}.csv\")\n",
    "        pp_sample = pd.read_csv(f\"../dataset/question_generation_samples/pp_{topic}.csv\")\n",
    "\n",
    "        fm_sample = fm_sample.sample(n=50, random_state=i).reset_index(drop=True)\n",
    "        mlb_sample = mlb_sample.sample(n=50, random_state=i).reset_index(drop=True)\n",
    "        pp_sample = pp_sample.sample(n=50, random_state=i).reset_index(drop=True)\n",
    "\n",
    "        comments = [\n",
    "            [str(row.get(\"Text\", \"\")).strip() for row in df.to_dict(\"records\")]\n",
    "            for df in [fm_sample, mlb_sample, pp_sample]\n",
    "        ]\n",
    "        prompt = format_prompt_with_all_comments(COMMUNITIES, comments, topic)\n",
    "\n",
    "        with open(os.path.join(output_dir, f\"{i}.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(prompt)\n",
    "        prompts.append(prompt)\n",
    "\n",
    "print(f\"Generated {len(prompts)} prompts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Questions Using All Three LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_txt_files_from_directory(directory):\n",
    "    txt_files = {}\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            with open(os.path.join(directory, filename), \"r\", encoding=\"utf-8\") as file:\n",
    "                txt_files[filename] = file.read()\n",
    "    return txt_files\n",
    "\n",
    "for topic in TOPIC_KEYWORDS:\n",
    "    prompts_directory = f\"../dataset/prompts/{topic}\"\n",
    "    txt_files = read_txt_files_from_directory(prompts_directory)\n",
    "\n",
    "    for model_type in [\"gemini\"]:\n",
    "        print(f\"\\nProcessing {topic} with {model_type}...\")\n",
    "        results = {}\n",
    "\n",
    "        for filename, content in tqdm(txt_files.items(), desc=f\"{model_type}\"):\n",
    "            response = call_llm(content, model_type=model_type)\n",
    "            results[filename] = response\n",
    "\n",
    "            output_dir = f\"../dataset/RQ1_questions/{topic}/output/{model_type}\"\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "            iteration_num = filename.replace(\".txt\", \"\")\n",
    "            output_filename = os.path.join(output_dir, f\"output_{iteration_num}.txt\")\n",
    "\n",
    "            with open(output_filename, \"w\", encoding=\"utf-8\") as output_file:\n",
    "                output_file.write(response)\n",
    "\n",
    "            time.sleep(5)\n",
    "\n",
    "        print(f\"Saved {len(results)} responses for {topic} using {model_type}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
